name: Python test

on:
  workflow_dispatch:
  schedule:
    - cron: '0 * * * *'

jobs:
  pytest:
    runs-on: [self-hosted, ubuntu_e2e]
    steps:
      - uses: actions/checkout@v2
      - uses: pdm-project/setup-pdm@v3
        name: Setup PDM
        with:
          python-version: 3.9  
          cache: true
          architecture: x64    
          version: head     
          prerelease: true     
          enable-pep582: true  
      - name: Install dependencies
        run: pdm install  
      - name: Run pytest
        run: |
          set -o pipefail
          pdm run -v pytest -vvvvvv --html=report.html --self-contained-html --junitxml=report.xml
#         continue-on-error: true
        id: pytest

      - name: Test Report 1
        uses: dorny/test-reporter@v1.6.0
        if: success() || failure()
        with:
          name: "Unit test report"
          fail-on-error: 'false'
          path: 'report.xml'
          reporter: java-junit  
      - name: Test Report 2
        uses: phoenix-actions/test-reporting@v10
        id: test-report 
        if: success() || failure()    # run this step even if previous step failed
        with:
          name: Python Tests            # Name of the check run which will be created
          path: report.xml    # Path to test results
          reporter: java-junit
          only-summary: 'false'
          list-suites: 'all'
          list-tests: 'all'
          fail-on-error: 'false'
          output-to: 'step-summary'
      - name: Read output variables
        if: always()
        run: |
          echo "url is ${{ steps.test-report.outputs.runHtmlUrl }}"
#       - name: Check test result
#         if: failure()
#         run: |
#           num_tests=$(jq ".num_tests" < pytest.json)
#           num_errors=$(jq ".num_errors" < pytest.json)
#           num_failures=$(jq ".num_failures" < pytest.json)
#           num_skipped=$(jq ".num_skipped" < pytest.json)
#           summary=$(jq ".summary" < pytest.json)
#           echo "Test failed"
#           echo "num_tests = $num_tests"
#           echo "num_errors = $num_errors"
#           echo "num_failures = $num_failures"
#           echo "num_skipped = $num_skipped"
#           echo "summary = $summary"
#         id: check-test-result
      - name: Upload Pytest HTML report
        if: always()
        uses: actions/upload-artifact@v1
        with:
          name: pytest-report
          path: report.html
          
      - name: Create or Update Issue
        if: failure()
        uses: JasonEtco/create-an-issue@v2
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          REPO: ${{ github.repository }}
          RUN_ID: ${{ github.run_id }}
          WORKFLOW: ${{ github.workflow }}
          TITLE: 'SNS API Testing Failed'
        with:
          filename: .github/TEST_FAIL_TEMPLATE.md
          update_existing: true

#       - name: Create or update test result issue
#         if: success() || failure()
#         uses: peter-evans/create-or-update-comment@v2
#         with:
#           title: Test Result
#           body: |
#             The test results are attached.
            
#             ## Test Summary

#             | Test Suite | Tests | Errors | Failures | Skipped |
#             |------------|-------|--------|----------|---------|
#             | All        |  ${{steps.pytest.outputs.num_tests}} | ${{steps.pytest.outputs.num_errors}} | ${{steps.pytest.outputs.num_failures}} | ${{steps.pytest.outputs.num_skipped}} |

#             ## Test Details

#             ${{steps.pytest.outputs.summary}}

#             ##Pytest HTML report
            
#             ${{ steps.upload-artifact.outputs.url }}
#           labels: test-result
#           issue_number: ${{github.event.issue.number}}


